{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import re\n",
    "import itertools\n",
    "from time import sleep\n",
    "import sys\n",
    "import numpy\n",
    "import scipy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter\n",
    "with open(\"proj1_data.json\") as fp:\n",
    "    data = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[:10000]\n",
    "validation = data[10000:11000]\n",
    "test = data[11000:12000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countAllFrequencyNaive(dataSet): \n",
    "    totalString = str()\n",
    "    for d in dataSet: \n",
    "        totalString = totalString + ' ' + d['text']\n",
    "    countNaive = Counter([s for s in totalString.lower().strip().split()])\n",
    "    totalCount = []\n",
    "    return list(map(lambda v: v[0], countNaive.most_common(160)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countAllFrequencyStopWord(dataSet): \n",
    "    \n",
    "    totalString = str()\n",
    "    \n",
    "    for d in dataSet: \n",
    "        totalString = totalString + ' ' + d['text']\n",
    "        \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    withoutPunc = tokenizer.tokenize(totalString)\n",
    "    \n",
    "    stopwordsSet = set(stopwords.words())\n",
    "    \n",
    "    countDict = Counter(s.lower() for s in withoutPunc if s.lower() not in stopwordsSet)\n",
    "    \n",
    "    return list(map(lambda v: v[0], countDict.most_common(160)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word count, no removal of punctuations\n",
    "# @Param: singleText: text to process, numberOfFeatures: 0, 60, 160?\n",
    "# @Return: vector of count (x in description)\n",
    "def wordCountNaive(singleText, numberOfFeatures, totalCount): \n",
    "    \n",
    "    countNaive = Counter([s.lower() for s in singleText.split()])\n",
    "    returnVector = []\n",
    "    for word in totalCount[:numberOfFeatures]: \n",
    "        returnVector.append(float(countNaive[word]))\n",
    "    return returnVector\n",
    "\n",
    "\n",
    "# word count, remove punc and stopwords to imporve model\n",
    "# @Param: singleText: text to process, numberOfFeatures: 0, 60, 160?\n",
    "# @Return: vector of count (x in description)\n",
    "def wordCountWithStopwords(singleText, numberOfFeatures, totalCount): \n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    withoutPunc = tokenizer.tokenize(singleText)\n",
    "    \n",
    "    stopwordsSet = set(stopwords.words())\n",
    "    \n",
    "    countDict = Counter([s.lower() for s in withoutPunc if s.lower() not in stopwordsSet])\n",
    "    \n",
    "    returnVector = []\n",
    "    for word in totalCount[:numberOfFeatures]: \n",
    "        returnVector.append(float(countDict[word]))\n",
    "    \n",
    "    return returnVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pythonListTranspose(xl): \n",
    "    return list(map(list, itertools.zip_longest(*xl)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Parser\n",
    "# @Param: dataVector: sliced original dataset, wordCountFunction: <str> -> ndarray<float>, \n",
    "# wordPOSFunction: str -> list<float>, numberOfTextFeature: 0 to shut down Text Processing (Text Features)\n",
    "# featureType: \n",
    "# @Return: tuple<ndarray, list>: xEngineered, yExtracted: rows: vector<samplePoint>, samplePoint[0->2]: basic Features, \n",
    "# samplePoint[3->162]: text, samplePoint[163->167]: extra\n",
    "def parseFeatures(dataVector, wordCountFunction, numberOfTextFeatures, featureType): \n",
    "    y = []\n",
    "    childrenFeature = []\n",
    "    controversialityFeature = []\n",
    "    isRootFeature = []\n",
    "    processedTextFeature = []\n",
    "    verbFeature = []\n",
    "    nounFeature = []\n",
    "    adjFeature = []\n",
    "    urlFeature = []\n",
    "    childAndControv = []\n",
    "    childAndisRoot = []\n",
    "    allInteracted = []\n",
    "    controvAndisRoot = []\n",
    "    identityFeature = []\n",
    "    c = 0\n",
    "    lenV = len(dataVector)\n",
    "    for dataPoint in dataVector: \n",
    "        \n",
    "        # Basic Features\n",
    "        y.append(float(dataPoint['popularity_score']))\n",
    "        if 'children' in featureType: \n",
    "            childrenFeature.append(float(dataPoint['children']))\n",
    "        if 'controv' in featureType: \n",
    "            controversialityFeature.append(float(dataPoint['controversiality']))\n",
    "        identityFeature.append(1.0)\n",
    "        isRootVar = -1.0\n",
    "        if 'isRoot' in featureType: \n",
    "            if dataPoint['is_root'] == True: \n",
    "                isRootVar = 1.0\n",
    "                isRootFeature.append(isRootVar)\n",
    "            else: \n",
    "                isRootVar = 0.0\n",
    "                isRootFeature.append(isRootVar)\n",
    "        \n",
    "        # Text Features: 0 to shut down text feature\n",
    "        if numberOfTextFeatures > 0 and 'text' in featureType: \n",
    "            processedTextFeature.append(wordCountFunction(dataPoint['text'], numberOfTextFeatures))\n",
    "        \n",
    "        # Extra Features\n",
    "        if 'noun' in featureType or 'verb' in featureType or 'adj' in featureType: \n",
    "            wordAnalysis = wordPOSCountWithStopwords(dataPoint['text'])\n",
    "        if 'verb' in featureType: \n",
    "            verbFeature.append(wordAnalysis[0])\n",
    "        if 'noun' in featureType: \n",
    "            nounFeature.append(wordAnalysis[1])\n",
    "        if 'adj' in featureType: \n",
    "            adjFeature.append(wordAnalysis[2])\n",
    "        if 'url' in featureType: \n",
    "            urlFeature.append(hasURL(dataPoint['text']))\n",
    "            \n",
    "        # Interaction Effect: \n",
    "        if 'child and controv' in featureType: \n",
    "            childAndControv.append(float(dataPoint['children']) * float(dataPoint['controversiality']))\n",
    "        if 'child and isRoot' in featureType: \n",
    "            childAndisRoot.append(float(dataPoint['children']) * isRootVar)\n",
    "        if 'all interacted' in featureType: \n",
    "            allInteracted.append(float(dataPoint['children']) * isRootVar  * float(dataPoint['controversiality']))\n",
    "        if 'controv and isRoot' in featureType: \n",
    "            controvAndisRoot.append(isRootVar  * float(dataPoint['controversiality']))\n",
    "        \n",
    "        # Process Bar\n",
    "        sys.stdout.write('\\r')\n",
    "        count = int((float(c) / float(lenV)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(count / 5), count))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        c = c + 1\n",
    "    if len(processedTextFeature) > 0: \n",
    "        processedTextFeature = pythonListTranspose(processedTextFeature)\n",
    "    returnTotal = [childrenFeature, controversialityFeature, isRootFeature, verbFeature, nounFeature, adjFeature, urlFeature, childAndControv, childAndisRoot, allInteracted, controvAndisRoot, identityFeature] + processedTextFeature\n",
    "    return pythonListTranspose([x for x in returnTotal if len(x) > 0]), pythonListTranspose([y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseTransformedFeatures(dataVector, wordCountFunction, numberOfTextFeatures, featureType): \n",
    "    y = []\n",
    "    childrenFeature = []\n",
    "    controversialityFeature = []\n",
    "    isRootFeature = []\n",
    "    processedTextFeature = []\n",
    "    verbFeature = []\n",
    "    nounFeature = []\n",
    "    adjFeature = []\n",
    "    urlFeature = []\n",
    "    childAndControv = []\n",
    "    childAndisRoot = []\n",
    "    allInteracted = []\n",
    "    controvAndisRoot = []\n",
    "    identityFeature = []\n",
    "    c = 0\n",
    "    lenV = len(dataVector)\n",
    "    for dataPoint in dataVector: \n",
    "        \n",
    "        # Basic Features\n",
    "        y.append(float(dataPoint['popularity_score']))\n",
    "        if 'children' in featureType: \n",
    "            childrenFeature.append(1.0 - numpy.exp(-0.04 * float(dataPoint['children'])))\n",
    "        if 'controv' in featureType: \n",
    "            controversialityFeature.append(float(dataPoint['controversiality']))\n",
    "        identityFeature.append(1.0)\n",
    "        isRootVar = -1.0\n",
    "        if 'isRoot' in featureType: \n",
    "            if dataPoint['is_root'] == True: \n",
    "                isRootVar = 1.0\n",
    "                isRootFeature.append(isRootVar)\n",
    "            else: \n",
    "                isRootVar = 0.0\n",
    "                isRootFeature.append(isRootVar)\n",
    "        \n",
    "        # Text Features: 0 to shut down text feature\n",
    "        if numberOfTextFeatures > 0 and 'text' in featureType: \n",
    "            processedTextFeature.append(wordCountFunction(dataPoint['text'], numberOfTextFeatures))\n",
    "        \n",
    "        # Extra Features\n",
    "        if 'noun' in featureType or 'verb' in featureType or 'adj' in featureType: \n",
    "            wordAnalysis = wordPOSCountWithStopwords(dataPoint['text'])\n",
    "        if 'verb' in featureType: \n",
    "            verbFeature.append(wordAnalysis[0])\n",
    "        if 'noun' in featureType: \n",
    "            nounFeature.append(wordAnalysis[1])\n",
    "        if 'adj' in featureType: \n",
    "            adjFeature.append(wordAnalysis[2])\n",
    "        if 'url' in featureType: \n",
    "            urlFeature.append(hasURL(dataPoint['text']))\n",
    "            \n",
    "        # Interaction Effect: \n",
    "        if 'child and controv' in featureType: \n",
    "            childAndControv.append(float(dataPoint['children']) * float(dataPoint['controversiality']))\n",
    "        if 'child and isRoot' in featureType: \n",
    "            childAndisRoot.append(float(dataPoint['children']) * isRootVar)\n",
    "        if 'all interacted' in featureType: \n",
    "            allInteracted.append(float(dataPoint['children']) * isRootVar  * float(dataPoint['controversiality']))\n",
    "        if 'controv and isRoot' in featureType: \n",
    "            controvAndisRoot.append(isRootVar  * float(dataPoint['controversiality']))\n",
    "        \n",
    "        # Process Bar\n",
    "        sys.stdout.write('\\r')\n",
    "        count = int((float(c) / float(lenV)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(count / 5), count))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        c = c + 1\n",
    "    if len(processedTextFeature) > 0: \n",
    "        processedTextFeature = pythonListTranspose(processedTextFeature)\n",
    "    returnTotal = [childrenFeature, controversialityFeature, isRootFeature, verbFeature, nounFeature, adjFeature, urlFeature, childAndControv, childAndisRoot, allInteracted, controvAndisRoot, identityFeature] + processedTextFeature\n",
    "    return pythonListTranspose([x for x in returnTotal if len(x) > 0]), pythonListTranspose([y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Regression Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanSquareError(valX, valY, w): \n",
    "    diffenence = numpy.power(numpy.transpose(numpy.subtract(valY, numpy.matmul(valX, w)))[0], 2)\n",
    "    return numpy.divide(numpy.sum(diffenence), len(valY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closedFormLinearRegression(x, y): \n",
    "    xT = numpy.transpose(numpy.array(x))\n",
    "    return numpy.matmul(numpy.matmul(scipy.linalg.inv(numpy.matmul(xT, numpy.array(x))), xT), numpy.array(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescentLinearRegression(learnRateFunction, x, y, tol): \n",
    "    i = 1\n",
    "    weight = numpy.array([[0.0] for l in range(len(x[0]))])\n",
    "    weightN = numpy.array([[0.0] for l in range(len(x[0]))])\n",
    "    xT = numpy.transpose(x)\n",
    "    xTx = numpy.matmul(xT, x)\n",
    "    xTy = numpy.matmul(xT, y)\n",
    "    while True: \n",
    "        weight = weightN\n",
    "        weightN = numpy.subtract(weight, 2 * learnRateFunction(i) * numpy.subtract(numpy.matmul(xTx, weight), xTy))\n",
    "        i = i + 1\n",
    "        if numpy.linalg.norm(numpy.subtract(weightN, weight), 2) <= tol: \n",
    "            break\n",
    "    return weightN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Closed Form and Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================== ] 99%"
     ]
    }
   ],
   "source": [
    "trainFeatures = parseFeatures(train, lambda u, v: wordCountNaive(u, v, tFreq), 0, ['children', 'controv', 'isRoot'])\n",
    "validationFeatures = parseFeatures(validation, lambda u, v: wordCountNaive(u, v, vFreq), 0, ['children', 'controv', 'isRoot'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result of closed form: \n",
      " [[ 0.37536403]\n",
      " [-1.08584747]\n",
      " [-0.22627679]\n",
      " [ 0.82092517]]\n",
      "error of closed form: \n",
      " 1.0203266848431447\n",
      "error of closed trained: \n",
      " 1.0846830709157251\n"
     ]
    }
   ],
   "source": [
    "resultClosed = closedFormLinearRegression(trainFeatures[0], trainFeatures[1])\n",
    "errorClosed = meanSquareError(validationFeatures[0], validationFeatures[1], resultClosed)\n",
    "errorTrained = meanSquareError(trainFeatures[0], trainFeatures[1], resultClosed)\n",
    "print('result of closed form: \\n', resultClosed)\n",
    "print('error of closed form: \\n', errorClosed)\n",
    "print('error of closed trained: \\n', errorTrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result of gradient descent: \n",
      " [[ 0.37533109]\n",
      " [-1.07323872]\n",
      " [-0.22619048]\n",
      " [ 0.8207526 ]]\n",
      "error of gradient descent: \n",
      " 1.020380306032347\n"
     ]
    }
   ],
   "source": [
    "resultGradient = gradientDescentLinearRegression(lambda v: float(0.0020 / (float(v) + 7.0)), trainFeatures[0], trainFeatures[1], 0.00000005)\n",
    "errorGradient = meanSquareError(validationFeatures[0], validationFeatures[1], resultGradient)\n",
    "print('result of gradient descent: \\n', resultGradient)\n",
    "print('error of gradient descent: \\n', errorGradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conclusion: closed form gives less error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: 60 and 160 text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================== ] 99%"
     ]
    }
   ],
   "source": [
    "tFreq = countAllFrequencyNaive(train)\n",
    "trainFeatures60 = parseFeatures(train, lambda u, v: wordCountNaive(u, v, tFreq), 60, ['children', 'controv', 'isRoot', 'text'])\n",
    "trainFeatures160 = parseFeatures(train, lambda u, v: wordCountNaive(u, v, tFreq), 160, ['children', 'controv', 'isRoot', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================== ] 99%"
     ]
    }
   ],
   "source": [
    "vFreq = countAllFrequencyNaive(validation)\n",
    "validationFeatures60 = parseFeatures(validation, lambda u, v: wordCountNaive(u, v, tFreq), 60, ['children', 'controv', 'isRoot', 'text'])\n",
    "validationFeatures160 = parseFeatures(validation, lambda u, v: wordCountNaive(u, v, tFreq), 160, ['children', 'controv', 'isRoot', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error of closed form: \n",
      " 0.9839397297217665\n",
      "error of closed trained: \n",
      " 1.060429141685383\n"
     ]
    }
   ],
   "source": [
    "resultClosed60 = closedFormLinearRegression(trainFeatures60[0], trainFeatures60[1])\n",
    "errorClosed60 = meanSquareError(validationFeatures60[0], validationFeatures60[1], resultClosed60)\n",
    "errorTrained60 = meanSquareError(trainFeatures60[0], trainFeatures60[1], resultClosed60)\n",
    "# print('result of closed form: \\n', resultClosed60)\n",
    "print('error of closed form: \\n', errorClosed60)\n",
    "print('error of closed trained: \\n', errorTrained60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error of closed form: \n",
      " 0.9950693970669264\n",
      "error of closed trained: \n",
      " 1.0477763217987115\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultClosed160 = closedFormLinearRegression(trainFeatures160[0], trainFeatures160[1])\n",
    "errorClosed160 = meanSquareError(validationFeatures160[0], validationFeatures160[1], resultClosed160)\n",
    "errorTrained160 = meanSquareError(trainFeatures160[0], trainFeatures160[1], resultClosed160)\n",
    "# print('result of closed form: \\n', resultClosed160)\n",
    "print('error of closed form: \\n', errorClosed160)\n",
    "print('error of closed trained: \\n', errorTrained160)\n",
    "len(trainFeatures160[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conclusion: 60 features are better than 160, validation error larger than closed form in 160: potentially overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra experiment: count word frequencies and eliminate stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================== ] 99%"
     ]
    }
   ],
   "source": [
    "tFreqS = countAllFrequencyStopWord(train)\n",
    "trainFeatures60S = parseFeatures(data[:10000], lambda u, v: wordCountWithStopwords(u, v, tFreqS), 60, ['children', 'controv', 'isRoot', 'text'])\n",
    "trainFeatures160S = parseFeatures(data[:10000], lambda u, v: wordCountWithStopwords(u, v, tFreqS), 160, ['children', 'controv', 'isRoot', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================== ] 99%"
     ]
    }
   ],
   "source": [
    "validationFeatures60S = parseFeatures(validation, lambda u, v: wordCountWithStopwords(u, v, tFreqS), 60, ['children', 'controv', 'isRoot', 'text'])\n",
    "validationFeatures160S = parseFeatures(validation, lambda u, v: wordCountWithStopwords(u, v, tFreqS), 160, ['children', 'controv', 'isRoot', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error of closed form: \n",
      " 1.0160834359569333\n",
      "error of closed trained: \n",
      " 1.0690435344142226\n"
     ]
    }
   ],
   "source": [
    "resultClosed60S = closedFormLinearRegression(trainFeatures60S[0], trainFeatures60S[1])\n",
    "errorClosed60S = meanSquareError(validationFeatures60S[0], validationFeatures60S[1], resultClosed60S)\n",
    "errorTrained60S = meanSquareError(trainFeatures60S[0], trainFeatures60S[1], resultClosed60S)\n",
    "# print('result of closed form: \\n', resultClosed60S)\n",
    "print('error of closed form: \\n', errorClosed60S)\n",
    "print('error of closed trained: \\n', errorTrained60S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error of closed form: \n",
      " 1.0377809673400562\n",
      "error of closed trained: \n",
      " 1.0504238189332942\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultClosed160S = closedFormLinearRegression(trainFeatures160S[0], trainFeatures160S[1])\n",
    "errorClosed160S = meanSquareError(validationFeatures160S[0], validationFeatures160S[1], resultClosed160S)\n",
    "errorTrained160S = meanSquareError(trainFeatures160S[0], trainFeatures160S[1], resultClosed160S)\n",
    "# print('result of closed form: \\n', resultClosed160S)\n",
    "print('error of closed form: \\n', errorClosed160S)\n",
    "print('error of closed trained: \\n', errorTrained160S)\n",
    "len(trainFeatures160[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conclusion: eliminating stopwords does not gives better result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with new features: fraction of noun and existence of URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word pos count, remove punc and stopwords to imporve model\n",
    "# @Param: singleText: text to process\n",
    "# @Return: vector of [verbcount, nouncount, adjcount]\n",
    "def wordPOSCountWithStopwords(singleText): \n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    withoutPunc = tokenizer.tokenize(singleText)\n",
    "    setOfStop = set(stopwords.words())\n",
    "    tagged = nltk.pos_tag([s.lower() for s in withoutPunc if s.lower() not in setOfStop])\n",
    "    \n",
    "    verbTotal, nounTotal, adjTotal = 0, 0, 0\n",
    "    counts = Counter(tag for wordType, tag in tagged)\n",
    "    totalCount = len(singleText.split())\n",
    "    \n",
    "    for key, value in counts.items(): \n",
    "        if 'NN' in key: \n",
    "            nounTotal = nounTotal + 1\n",
    "        elif 'VB' in key: \n",
    "            verbTotal = verbTotal + 1\n",
    "        elif 'JJ' in key: \n",
    "            adjTotal = adjTotal + 1\n",
    "            \n",
    "    if totalCount > 0: \n",
    "        return [float(verbTotal) / totalCount, float(nounTotal) / totalCount, float(adjTotal) / totalCount]\n",
    "    else: \n",
    "        return [0.0, 0.0, 0.0]\n",
    "# Test: \n",
    "# print(wordPOSCountWithStopwords(data[3]['text']))\n",
    "# print(data[3]['text'])\n",
    "\n",
    "\n",
    "def hasURL(text): \n",
    "    if re.match(r\"(http://[^ ]+)\", text) != None: \n",
    "        return 1.0\n",
    "    else: \n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With fraction of noun in whole words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================== ] 99%"
     ]
    }
   ],
   "source": [
    "trainMyFeatures60 = parseFeatures(train, lambda u, v: wordCountNaive(u, v, tFreq), 60, ['children', 'controv', 'isRoot', 'noun'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================== ] 99%"
     ]
    }
   ],
   "source": [
    "# vFreq = countAllFrequencyNaive(validation)\n",
    "validationMyFeatures60 = parseFeatures(validation, lambda u, v: wordCountNaive(u, v, tFreq), 60, ['children', 'controv', 'isRoot', 'noun'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error of closed form: \n",
      " 1.0205294940039473\n",
      "error of closed trained: \n",
      " 1.0846431658913824\n"
     ]
    }
   ],
   "source": [
    "resultMyClosed = closedFormLinearRegression(trainMyFeatures60[0], trainMyFeatures60[1])\n",
    "errorMyClosed = meanSquareError(validationMyFeatures60[0], validationMyFeatures60[1], resultMyClosed)\n",
    "errorMyTrained = meanSquareError(trainMyFeatures60[0], trainMyFeatures60[1], resultMyClosed)\n",
    "# print('result of closed form: \\n', resultMyClosed)\n",
    "print('error of closed form: \\n', errorMyClosed)\n",
    "print('error of closed trained: \\n', errorMyTrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================== ] 99%"
     ]
    }
   ],
   "source": [
    "trainMyFeaturesURL60 = parseFeatures(train, lambda u, v: wordCountNaive(u, v, tFreq), 60, ['children', 'controv', 'isRoot', 'url'])\n",
    "validationMyFeaturesURL60 = parseFeatures(validation, lambda u, v: wordCountNaive(u, v, tFreq), 60, ['children', 'controv', 'isRoot', 'url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error of closed form: \n",
      " 1.0206699968316335\n",
      "error of closed trained: \n",
      " 1.0846292895663272\n"
     ]
    }
   ],
   "source": [
    "resultMyClosedURL = closedFormLinearRegression(trainMyFeaturesURL60[0], trainMyFeaturesURL60[1])\n",
    "errorMyClosedURL = meanSquareError(validationMyFeaturesURL60[0], validationMyFeaturesURL60[1], resultMyClosedURL)\n",
    "errorMyTrainedURL = meanSquareError(trainMyFeaturesURL60[0], trainMyFeaturesURL60[1], resultMyClosedURL)\n",
    "# print('result of closed form: \\n', resultMyClosedURL)\n",
    "print('error of closed form: \\n', errorMyClosedURL)\n",
    "print('error of closed trained: \\n', errorMyTrainedURL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================== ] 99%"
     ]
    }
   ],
   "source": [
    "trainMyFeaturesALL60 = parseFeatures(train, lambda u, v: wordCountNaive(u, v, tFreq), 60, ['children', 'controv', 'isRoot', 'noun', 'url'])\n",
    "validationMyFeaturesALL60 = parseFeatures(validation, lambda u, v: wordCountNaive(u, v, tFreq), 60, ['children', 'controv', 'isRoot', 'noun', 'url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error of closed form: \n",
      " 1.0208223075830674\n",
      "error of closed trained: \n",
      " 1.084599876507505\n"
     ]
    }
   ],
   "source": [
    "resultMyClosedALL = closedFormLinearRegression(trainMyFeaturesALL60[0], trainMyFeaturesALL60[1])\n",
    "errorMyClosedALL = meanSquareError(validationMyFeaturesALL60[0], validationMyFeaturesALL60[1], resultMyClosedALL)\n",
    "errorMyTrainedALL = meanSquareError(trainMyFeaturesALL60[0], trainMyFeaturesALL60[1], resultMyClosedALL)\n",
    "# print('result of closed form: \\n', resultMyClosedALL)\n",
    "print('error of closed form: \\n', errorMyClosedALL)\n",
    "print('error of closed trained: \\n', errorMyTrainedALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conclusion: adding extra features does not help with accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x4(noun) and x5(url) has no significant coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test of coefficient (Hastie. et, al)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.206</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.206</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   518.3</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 21 Jan 2019</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>15:26:42</td>     <th>  Log-Likelihood:    </th> <td> -14595.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 10000</td>      <th>  AIC:               </th> <td>2.920e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  9994</td>      <th>  BIC:               </th> <td>2.925e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    0.3755</td> <td>    0.008</td> <td>   48.207</td> <td> 0.000</td> <td>    0.360</td> <td>    0.391</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>   -1.0856</td> <td>    0.097</td> <td>  -11.195</td> <td> 0.000</td> <td>   -1.276</td> <td>   -0.896</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>   -0.2272</td> <td>    0.021</td> <td>  -10.710</td> <td> 0.000</td> <td>   -0.269</td> <td>   -0.186</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>    0.0246</td> <td>    0.047</td> <td>    0.521</td> <td> 0.603</td> <td>   -0.068</td> <td>    0.117</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>    0.2212</td> <td>    0.350</td> <td>    0.632</td> <td> 0.528</td> <td>   -0.465</td> <td>    0.908</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    0.8172</td> <td>    0.016</td> <td>   51.946</td> <td> 0.000</td> <td>    0.786</td> <td>    0.848</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>2161.667</td> <th>  Durbin-Watson:     </th> <td>   2.022</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>23058.853</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 0.730</td>  <th>  Prob(JB):          </th> <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>10.295</td>  <th>  Cond. No.          </th> <td>    49.4</td> \n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.206\n",
       "Model:                            OLS   Adj. R-squared:                  0.206\n",
       "Method:                 Least Squares   F-statistic:                     518.3\n",
       "Date:                Mon, 21 Jan 2019   Prob (F-statistic):               0.00\n",
       "Time:                        15:26:42   Log-Likelihood:                -14595.\n",
       "No. Observations:               10000   AIC:                         2.920e+04\n",
       "Df Residuals:                    9994   BIC:                         2.925e+04\n",
       "Df Model:                           5                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "x1             0.3755      0.008     48.207      0.000       0.360       0.391\n",
       "x2            -1.0856      0.097    -11.195      0.000      -1.276      -0.896\n",
       "x3            -0.2272      0.021    -10.710      0.000      -0.269      -0.186\n",
       "x4             0.0246      0.047      0.521      0.603      -0.068       0.117\n",
       "x5             0.2212      0.350      0.632      0.528      -0.465       0.908\n",
       "const          0.8172      0.016     51.946      0.000       0.786       0.848\n",
       "==============================================================================\n",
       "Omnibus:                     2161.667   Durbin-Watson:                   2.022\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            23058.853\n",
       "Skew:                           0.730   Prob(JB):                         0.00\n",
       "Kurtosis:                      10.295   Cond. No.                         49.4\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "sm.OLS(trainMyFeaturesALL60[1], trainMyFeaturesALL60[0]).fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Improved!) Interaction Effect among basic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================== ] 99%"
     ]
    }
   ],
   "source": [
    "trainFeaturesI = parseTransformedFeatures(train, lambda u, v: wordCountNaive(u, v, tFreq), 0, ['children', 'controv', 'isRoot', 'child and controv', 'child and isRoot', 'all interacted', 'controv and isRoot'])\n",
    "validationFeaturesI = parseTransformedFeatures(validation, lambda u, v: wordCountNaive(u, v, tFreq), 0, ['children', 'controv', 'isRoot', 'child and controv', 'child and isRoot', 'all interacted', 'controv and isRoot']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result of closed form: \n",
      " [[14.32482055]\n",
      " [-0.68521679]\n",
      " [-0.18567247]\n",
      " [-0.54724021]\n",
      " [-0.02352103]\n",
      " [ 0.33900498]\n",
      " [-0.32725931]\n",
      " [ 0.74635056]]\n",
      "error of closed form: \n",
      " 1.00268783014828\n",
      "error of closed trained: \n",
      " 1.042929619951173\n"
     ]
    }
   ],
   "source": [
    "resultClosedI = closedFormLinearRegression(trainFeaturesI[0], trainFeaturesI[1])\n",
    "errorClosedI = meanSquareError(validationFeaturesI[0], validationFeaturesI[1], resultClosedI)\n",
    "errorTrainedI = meanSquareError(trainFeaturesI[0], trainFeaturesI[1], resultClosedI)\n",
    "print('result of closed form: \\n', resultClosedI)\n",
    "print('error of closed form: \\n', errorClosedI)\n",
    "print('error of closed trained: \\n', errorTrainedI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: use of g(x) = 1-exp(-0.04x) to transform children variable, and interact the transformed children with isRoot decreases loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.236</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.236</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   442.0</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 21 Jan 2019</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>15:27:00</td>     <th>  Log-Likelihood:    </th> <td> -14400.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 10000</td>      <th>  AIC:               </th> <td>2.882e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  9992</td>      <th>  BIC:               </th> <td>2.887e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     7</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>   14.3248</td> <td>    0.355</td> <td>   40.364</td> <td> 0.000</td> <td>   13.629</td> <td>   15.020</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>   -0.6852</td> <td>    0.150</td> <td>   -4.576</td> <td> 0.000</td> <td>   -0.979</td> <td>   -0.392</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>   -0.1857</td> <td>    0.022</td> <td>   -8.543</td> <td> 0.000</td> <td>   -0.228</td> <td>   -0.143</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>   -0.5472</td> <td>    0.113</td> <td>   -4.840</td> <td> 0.000</td> <td>   -0.769</td> <td>   -0.326</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>   -0.0235</td> <td>    0.015</td> <td>   -1.600</td> <td> 0.110</td> <td>   -0.052</td> <td>    0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>    <td>    0.3390</td> <td>    0.260</td> <td>    1.302</td> <td> 0.193</td> <td>   -0.172</td> <td>    0.850</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>    <td>   -0.3273</td> <td>    0.285</td> <td>   -1.150</td> <td> 0.250</td> <td>   -0.885</td> <td>    0.231</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    0.7464</td> <td>    0.015</td> <td>   50.254</td> <td> 0.000</td> <td>    0.717</td> <td>    0.775</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>1948.028</td> <th>  Durbin-Watson:     </th> <td>   2.023</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>18201.749</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 0.666</td>  <th>  Prob(JB):          </th> <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td> 9.474</td>  <th>  Cond. No.          </th> <td>    41.1</td> \n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.236\n",
       "Model:                            OLS   Adj. R-squared:                  0.236\n",
       "Method:                 Least Squares   F-statistic:                     442.0\n",
       "Date:                Mon, 21 Jan 2019   Prob (F-statistic):               0.00\n",
       "Time:                        15:27:00   Log-Likelihood:                -14400.\n",
       "No. Observations:               10000   AIC:                         2.882e+04\n",
       "Df Residuals:                    9992   BIC:                         2.887e+04\n",
       "Df Model:                           7                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "x1            14.3248      0.355     40.364      0.000      13.629      15.020\n",
       "x2            -0.6852      0.150     -4.576      0.000      -0.979      -0.392\n",
       "x3            -0.1857      0.022     -8.543      0.000      -0.228      -0.143\n",
       "x4            -0.5472      0.113     -4.840      0.000      -0.769      -0.326\n",
       "x5            -0.0235      0.015     -1.600      0.110      -0.052       0.005\n",
       "x6             0.3390      0.260      1.302      0.193      -0.172       0.850\n",
       "x7            -0.3273      0.285     -1.150      0.250      -0.885       0.231\n",
       "const          0.7464      0.015     50.254      0.000       0.717       0.775\n",
       "==============================================================================\n",
       "Omnibus:                     1948.028   Durbin-Watson:                   2.023\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            18201.749\n",
       "Skew:                           0.666   Prob(JB):                         0.00\n",
       "Kurtosis:                       9.474   Cond. No.                         41.1\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.OLS(trainFeaturesI[1], trainFeaturesI[0]).fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conclusion: interaction terms are not significant (all p-values are not smaller than 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra experiment: Forward Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================== ] 99%"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "trainMyFeatures60 = parseTransformedFeatures(train, lambda u, v: wordCountNaive(u, v, tFreq), 160, ['children', 'controv', 'isRoot', 'child and controv', 'text']) # , 'child and controv', 'text'\n",
    "validationMyFeatures60 = parseTransformedFeatures(validation, lambda u, v: wordCountNaive(u, v, tFreq), 160, ['children', 'controv', 'child and controv', 'isRoot', 'text'])\n",
    "testMyFeatures60 = parseTransformedFeatures(test, lambda u, v: wordCountNaive(u, v, tFreq), 160, ['children', 'controv', 'isRoot', 'child and controv', 'text'])\n",
    "\n",
    "\n",
    "# , 'noun', 'url'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 165), (1000, 165))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindf = pandas.DataFrame(trainMyFeatures60[0])\n",
    "validationdf = pandas.DataFrame(validationMyFeatures60[0])\n",
    "traindf.shape, validationdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================== ] 98%"
     ]
    }
   ],
   "source": [
    "fullPredictors = set([i for i in range(0, 165)])\n",
    "levelBest = [[sys.float_info.max, set()]]\n",
    "c = 0\n",
    "for k in range(1, len(fullPredictors)): \n",
    "    \n",
    "    prevBest = levelBest[-1][1]\n",
    "    currentPredictors = fullPredictors - prevBest\n",
    "    mseVal = []\n",
    "    \n",
    "    for predictor in currentPredictors: \n",
    "        selected = prevBest | set([predictor])\n",
    "        \n",
    "        trainX = traindf[list(selected)].values\n",
    "        validX = validationdf[list(selected)].values\n",
    "        trainY = trainMyFeatures60[1]\n",
    "        validY = validationMyFeatures60[1]\n",
    "        \n",
    "        try: \n",
    "            resultW = closedFormLinearRegression(trainX, trainY)\n",
    "        except: \n",
    "            continue\n",
    "        mseVal.append([meanSquareError(validX, validY, resultW), selected])\n",
    "    \n",
    "    if len(mseVal) > 0: \n",
    "        levelBest.append(min(mseVal, key = lambda x: x[1]))\n",
    "    \n",
    "    # Process Bar\n",
    "    sys.stdout.write('\\r')\n",
    "    count = int((float(c) / float(len(fullPredictors))) * 100)\n",
    "    sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(count / 5), count))\n",
    "    sleep(0.001)\n",
    "    sys.stdout.flush()\n",
    "    c = c + 1\n",
    "\n",
    "trainX = traindf[list(fullPredictors)].values\n",
    "validX = validationdf[list(fullPredictors)].values\n",
    "trainY = trainMyFeatures60[1]\n",
    "validY = validationMyFeatures60[1]\n",
    "ok = True\n",
    "try: \n",
    "    resultW = closedFormLinearRegression(trainX, trainY)\n",
    "except: \n",
    "    ok = False\n",
    "if ok: \n",
    "    mseVal.append([meanSquareError(validX, validY, resultW), selected])\n",
    "bestOfAll = sorted(levelBest, key = lambda v: v[0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best set of predictors with interaction term: \n",
      " 0.9517568801197965\n",
      "error: \n",
      " {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65}\n"
     ]
    }
   ],
   "source": [
    "print('best set of predictors with interaction term: \\n', bestOfAll[0])\n",
    "print('error: \\n', bestOfAll[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.271401600017087"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultClosedFinal = closedFormLinearRegression(numpy.array(trainMyFeatures60[0])[:,[i for i in range(0, 66)]], trainMyFeatures60[1])\n",
    "meanSquareError(numpy.array(testMyFeatures60[0])[:,[i for i in range(0, 66)]], testMyFeatures60[1], resultClosedFinal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: overfits validation set (meta-overfitting)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
