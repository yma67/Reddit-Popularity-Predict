{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import re\n",
    "import itertools\n",
    "from time import sleep\n",
    "import sys\n",
    "import numpy\n",
    "import scipy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter\n",
    "with open(\"proj1_data.json\") as fp:\n",
    "    data = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[:10000]\n",
    "validation = data[10000:11000]\n",
    "test = data[11000:12000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countAllFrequencyNaive(dataSet): \n",
    "    totalString = str()\n",
    "    for d in dataSet: \n",
    "        totalString = totalString + ' ' + d['text']\n",
    "    countNaive = Counter([s for s in totalString.lower().strip().split()])\n",
    "    totalCount = []\n",
    "    return list(map(lambda v: v[0], countNaive.most_common(160)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countAllFrequencyStopWord(dataSet): \n",
    "    \n",
    "    totalString = str()\n",
    "    \n",
    "    for d in dataSet: \n",
    "        totalString = totalString + ' ' + d['text']\n",
    "        \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    withoutPunc = tokenizer.tokenize(totalString)\n",
    "    \n",
    "    stopwordsSet = set(stopwords.words())\n",
    "    \n",
    "    countDict = Counter(s.lower() for s in withoutPunc if s.lower() not in stopwordsSet)\n",
    "    \n",
    "    return list(map(lambda v: v[0], countDict.most_common(160)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word count, no removal of punctuations\n",
    "# @Param: singleText: text to process, numberOfFeatures: 0, 60, 160?\n",
    "# @Return: vector of count (x in description)\n",
    "def wordCountNaive(singleText, numberOfFeatures, totalCount): \n",
    "    \n",
    "    countNaive = Counter([s.lower() for s in singleText.split()])\n",
    "    returnVector = []\n",
    "    for word in totalCount[:numberOfFeatures]: \n",
    "        returnVector.append(float(countNaive[word]))\n",
    "    return returnVector\n",
    "\n",
    "\n",
    "# word count, remove punc and stopwords to imporve model\n",
    "# @Param: singleText: text to process, numberOfFeatures: 0, 60, 160?\n",
    "# @Return: vector of count (x in description)\n",
    "def wordCountWithStopwords(singleText, numberOfFeatures, totalCount): \n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    withoutPunc = tokenizer.tokenize(singleText)\n",
    "    \n",
    "    stopwordsSet = set(stopwords.words())\n",
    "    \n",
    "    countDict = Counter([s.lower() for s in withoutPunc if s.lower() not in stopwordsSet])\n",
    "    \n",
    "    returnVector = []\n",
    "    for word in totalCount[:numberOfFeatures]: \n",
    "        returnVector.append(float(countDict[word]))\n",
    "    \n",
    "    return returnVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pythonListTranspose(xl): \n",
    "    return list(map(list, itertools.zip_longest(*xl)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is used for calculate the ratio between unique char and num of words in a comment\n",
    "def charWordRatio(comment):\n",
    "    count = []\n",
    "    for c in comment:\n",
    "        if c not in count and ((c >= 'a' and c <= 'z')or( c >= 'A' and c <= 'Z')):\n",
    "            count.append(c)\n",
    "    return len(count)/len(comment.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniqueChar(comment):\n",
    "    count = []\n",
    "    for c in comment:\n",
    "        if c not in count and ((c >= 'a' and c <= 'z')or( c >= 'A' and c <= 'Z')):\n",
    "            count.append(c)\n",
    "    return len(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Parser\n",
    "# @Param: dataVector: sliced original dataset, wordCountFunction: <str> -> ndarray<float>, \n",
    "# wordPOSFunction: str -> list<float>, numberOfTextFeature: 0 to shut down Text Processing (Text Features)\n",
    "# featureType: \n",
    "# @Return: tuple<ndarray, list>: xEngineered, yExtracted: rows: vector<samplePoint>, samplePoint[0->2]: basic Features, \n",
    "# samplePoint[3->162]: text, samplePoint[163->167]: extra\n",
    "def parseFeatures(dataVector, wordCountFunction, numberOfTextFeatures, featureType): \n",
    "    y = []\n",
    "    childrenFeature = []\n",
    "    controversialityFeature = []\n",
    "    isRootFeature = []\n",
    "    processedTextFeature = []\n",
    "    verbFeature = []\n",
    "    nounFeature = []\n",
    "    adjFeature = []\n",
    "    urlFeature = []\n",
    "    childAndControv = []\n",
    "    childAndisRoot = []\n",
    "    allInteracted = []\n",
    "    controvAndisRoot = []\n",
    "    identityFeature = []\n",
    "    # charWordRatioFeature = []\n",
    "    # uniqueCharFeature = []\n",
    "    \n",
    "    c = 0\n",
    "    lenV = len(dataVector)\n",
    "    for dataPoint in dataVector: \n",
    "        \n",
    "        # Basic Features\n",
    "        y.append(float(dataPoint['popularity_score']))\n",
    "        if 'children' in featureType: \n",
    "            childrenFeature.append(float(dataPoint['children']))\n",
    "        if 'controv' in featureType: \n",
    "            controversialityFeature.append(float(dataPoint['controversiality']))\n",
    "        identityFeature.append(1.0)\n",
    "        isRootVar = -1.0\n",
    "        if 'isRoot' in featureType: \n",
    "            if dataPoint['is_root'] == True: \n",
    "                isRootVar = 1.0\n",
    "                isRootFeature.append(isRootVar)\n",
    "            else: \n",
    "                isRootVar = 0.0\n",
    "                isRootFeature.append(isRootVar)\n",
    "        \n",
    "        # Text Features: 0 to shut down text feature\n",
    "        if numberOfTextFeatures > 0 and 'text' in featureType: \n",
    "            processedTextFeature.append(wordCountFunction(dataPoint['text'], numberOfTextFeatures))\n",
    "        \n",
    "        # Extra Features\n",
    "        if 'noun' in featureType or 'verb' in featureType or 'adj' in featureType: \n",
    "            wordAnalysis = wordPOSCountWithStopwords(dataPoint['text'])\n",
    "        if 'verb' in featureType: \n",
    "            verbFeature.append(wordAnalysis[0])\n",
    "        if 'noun' in featureType: \n",
    "            nounFeature.append(wordAnalysis[1])\n",
    "        if 'adj' in featureType: \n",
    "            adjFeature.append(wordAnalysis[2])\n",
    "        if 'url' in featureType: \n",
    "            urlFeature.append(hasURL(dataPoint['text']))\n",
    "        \n",
    "        # charWordRatioFeature.append(charWordRatio(dataPoint['text']))\n",
    "        # uniqueCharFeature.append(uniqueChar(dataPoint['text']))\n",
    "        \n",
    "        # Interaction Effect: \n",
    "        if 'child and controv' in featureType: \n",
    "            childAndControv.append(float(dataPoint['children']) * float(dataPoint['controversiality']))\n",
    "        if 'child and isRoot' in featureType: \n",
    "            childAndisRoot.append(float(dataPoint['children']) * isRootVar)\n",
    "        if 'all interacted' in featureType: \n",
    "            allInteracted.append(float(dataPoint['children']) * isRootVar  * float(dataPoint['controversiality']))\n",
    "        if 'controv and isRoot' in featureType: \n",
    "            controvAndisRoot.append(isRootVar  * float(dataPoint['controversiality']))\n",
    "        \n",
    "        # Process Bar\n",
    "        sys.stdout.write('\\r')\n",
    "        count = int((float(c) / float(lenV)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(count / 5), count))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        c = c + 1\n",
    "    if len(processedTextFeature) > 0: \n",
    "        processedTextFeature = pythonListTranspose(processedTextFeature)\n",
    "    returnTotal = [childrenFeature, controversialityFeature, isRootFeature, verbFeature, nounFeature, adjFeature, urlFeature, childAndControv, childAndisRoot, allInteracted, controvAndisRoot, identityFeature] + processedTextFeature\n",
    "    return pythonListTranspose([x for x in returnTotal if len(x) > 0]), pythonListTranspose([y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseTransformedFeatures(dataVector, wordCountFunction, numberOfTextFeatures, featureType): \n",
    "    y = []\n",
    "    childrenFeature = []\n",
    "    controversialityFeature = []\n",
    "    isRootFeature = []\n",
    "    processedTextFeature = []\n",
    "    verbFeature = []\n",
    "    nounFeature = []\n",
    "    adjFeature = []\n",
    "    urlFeature = []\n",
    "    childAndControv = []\n",
    "    childAndisRoot = []\n",
    "    allInteracted = []\n",
    "    controvAndisRoot = []\n",
    "    identityFeature = []\n",
    "    # charWordRatioFeature = []\n",
    "    # uniqueCharFeature = []\n",
    "    \n",
    "    c = 0\n",
    "    lenV = len(dataVector)\n",
    "    for dataPoint in dataVector: \n",
    "        \n",
    "        # Basic Features\n",
    "        y.append(float(dataPoint['popularity_score']))\n",
    "        if 'children' in featureType: \n",
    "            childrenFeature.append(1.0 - numpy.exp(-0.04 * float(dataPoint['children'])))\n",
    "        if 'controv' in featureType: \n",
    "            controversialityFeature.append(float(dataPoint['controversiality']))\n",
    "        identityFeature.append(1.0)\n",
    "        isRootVar = -1.0\n",
    "        if 'isRoot' in featureType: \n",
    "            if dataPoint['is_root'] == True: \n",
    "                isRootVar = 1.0\n",
    "                isRootFeature.append(isRootVar)\n",
    "            else: \n",
    "                isRootVar = 0.0\n",
    "                isRootFeature.append(isRootVar)\n",
    "        \n",
    "        # Text Features: 0 to shut down text feature\n",
    "        if numberOfTextFeatures > 0 and 'text' in featureType: \n",
    "            processedTextFeature.append([numpy.exp(-0.04 * (a)) * numpy.cos(3 * a) for a in wordCountFunction(dataPoint['text'], numberOfTextFeatures)])\n",
    "            #  \n",
    "        \n",
    "        # Extra Features\n",
    "        if 'noun' in featureType or 'verb' in featureType or 'adj' in featureType: \n",
    "            wordAnalysis = wordPOSCountWithStopwords(dataPoint['text'])\n",
    "        if 'verb' in featureType: \n",
    "            verbFeature.append(wordAnalysis[0])\n",
    "        if 'noun' in featureType: \n",
    "            nounFeature.append(wordAnalysis[1])\n",
    "        if 'adj' in featureType: \n",
    "            adjFeature.append(wordAnalysis[2])\n",
    "        if 'url' in featureType: \n",
    "            urlFeature.append(hasURL(dataPoint['text']))\n",
    "            \n",
    "        # Interaction Effect: \n",
    "        if 'child and controv' in featureType: \n",
    "            childAndControv.append(float(dataPoint['children']) * float(dataPoint['controversiality']))\n",
    "        if 'child and isRoot' in featureType: \n",
    "            childAndisRoot.append(float(dataPoint['children']) * isRootVar)\n",
    "        if 'all interacted' in featureType: \n",
    "            allInteracted.append(float(dataPoint['children']) * isRootVar * float(dataPoint['controversiality']))\n",
    "        if 'controv and isRoot' in featureType: \n",
    "            controvAndisRoot.append(isRootVar  * float(dataPoint['controversiality']))\n",
    "        \n",
    "        # charWordRatioFeature.append(charWordRatio(dataPoint['text']))\n",
    "        # uniqueCharFeature.append(uniqueChar(dataPoint['text']))\n",
    "        \n",
    "        # Process Bar\n",
    "        sys.stdout.write('\\r')\n",
    "        count = int((float(c) / float(lenV)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(count / 5), count))\n",
    "        sleep(0.001)\n",
    "        sys.stdout.flush()\n",
    "        c = c + 1\n",
    "    \n",
    "    if len(processedTextFeature) > 0: \n",
    "        processedTextFeature = pythonListTranspose(processedTextFeature)\n",
    "        \n",
    "    returnTotal = [childrenFeature, controversialityFeature, isRootFeature, verbFeature, nounFeature, adjFeature, urlFeature, childAndControv, childAndisRoot, allInteracted, controvAndisRoot, identityFeature] + processedTextFeature\n",
    "    return pythonListTranspose([x for x in returnTotal if len(x) > 0]), pythonListTranspose([y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Regression Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanSquareError(valX, valY, w): \n",
    "    diffenence = numpy.power(numpy.transpose(numpy.subtract(valY, numpy.matmul(valX, w)))[0], 2)\n",
    "    return numpy.divide(numpy.sum(diffenence), len(valY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closedFormLinearRegression(x, y): \n",
    "    xT = numpy.transpose(numpy.array(x))\n",
    "    return numpy.matmul(numpy.matmul(scipy.linalg.inv(numpy.matmul(xT, numpy.array(x))), xT), numpy.array(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescentLinearRegression(learnRateFunction, x, y, tol): \n",
    "    i = 1\n",
    "    weight = numpy.array([[0.0] for l in range(len(x[0]))])\n",
    "    weightN = numpy.array([[0.0] for l in range(len(x[0]))])\n",
    "    xT = numpy.transpose(x)\n",
    "    xTx = numpy.matmul(xT, x)\n",
    "    xTy = numpy.matmul(xT, y)\n",
    "    while True: \n",
    "        weight = weightN\n",
    "        weightN = numpy.subtract(weight, 2 * learnRateFunction(i) * numpy.subtract(numpy.matmul(xTx, weight), xTy))\n",
    "        i = i + 1\n",
    "        if numpy.linalg.norm(numpy.subtract(weightN, weight), 2) <= tol: \n",
    "            break\n",
    "    return weightN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Closed Form and Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================== ] 99%"
     ]
    }
   ],
   "source": [
    "trainFeatures = parseFeatures(train, lambda u, v: wordCountNaive(u, v, tFreq), 0, ['children', 'controv', 'isRoot'])\n",
    "validationFeatures = parseFeatures(validation, lambda u, v: wordCountNaive(u, v, vFreq), 0, ['children', 'controv', 'isRoot'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result of closed form: \n",
      " [[ 0.37536403]\n",
      " [-1.08584747]\n",
      " [-0.22627679]\n",
      " [ 0.82092517]]\n",
      "error of closed form: \n",
      " 1.0203266848431447\n",
      "error of closed trained: \n",
      " 1.0846830709157251\n"
     ]
    }
   ],
   "source": [
    "resultClosed = closedFormLinearRegression(trainFeatures[0], trainFeatures[1])\n",
    "errorClosed = meanSquareError(validationFeatures[0], validationFeatures[1], resultClosed)\n",
    "errorTrained = meanSquareError(trainFeatures[0], trainFeatures[1], resultClosed)\n",
    "print('result of closed form: \\n', resultClosed)\n",
    "print('error of closed form: \\n', errorClosed)\n",
    "print('error of closed trained: \\n', errorTrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result of gradient descent: \n",
      " [[ 0.37533096]\n",
      " [-1.07318692]\n",
      " [-0.22619012]\n",
      " [ 0.82075189]]\n",
      "error of gradient descent: \n",
      " 1.020380531559443\n"
     ]
    }
   ],
   "source": [
    "resultGradient = gradientDescentLinearRegression(lambda v: float(0.0020 / (float(v) + 7.0)), trainFeatures[0], trainFeatures[1], 0.00000005)\n",
    "errorGradient = meanSquareError(validationFeatures[0], validationFeatures[1], resultGradient)\n",
    "print('result of gradient descent: \\n', resultGradient)\n",
    "print('error of gradient descent: \\n', errorGradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conclusion: closed form gives less error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: 60 and 160 text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================== ] 99%"
     ]
    }
   ],
   "source": [
    "tFreq = countAllFrequencyNaive(train)\n",
    "trainFeatures60 = parseFeatures(train, lambda u, v: wordCountNaive(u, v, tFreq), 60, ['children', 'controv', 'isRoot', 'text'])\n",
    "trainFeatures160 = parseFeatures(train, lambda u, v: wordCountNaive(u, v, tFreq), 160, ['children', 'controv', 'isRoot', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================== ] 99%"
     ]
    }
   ],
   "source": [
    "vFreq = countAllFrequencyNaive(validation)\n",
    "validationFeatures60 = parseFeatures(validation, lambda u, v: wordCountNaive(u, v, tFreq), 60, ['children', 'controv', 'isRoot', 'text'])\n",
    "validationFeatures160 = parseFeatures(validation, lambda u, v: wordCountNaive(u, v, tFreq), 160, ['children', 'controv', 'isRoot', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error of closed form: \n",
      " 0.9839397297217666\n",
      "error of closed trained: \n",
      " 1.060429141685383\n"
     ]
    }
   ],
   "source": [
    "resultClosed60 = closedFormLinearRegression(trainFeatures60[0], trainFeatures60[1])\n",
    "errorClosed60 = meanSquareError(validationFeatures60[0], validationFeatures60[1], resultClosed60)\n",
    "errorTrained60 = meanSquareError(trainFeatures60[0], trainFeatures60[1], resultClosed60)\n",
    "# print('result of closed form: \\n', resultClosed60)\n",
    "print('error of closed form: \\n', errorClosed60)\n",
    "print('error of closed trained: \\n', errorTrained60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error of closed form: \n",
      " 0.9950693970669265\n",
      "error of closed trained: \n",
      " 1.0477763217987115\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultClosed160 = closedFormLinearRegression(trainFeatures160[0], trainFeatures160[1])\n",
    "errorClosed160 = meanSquareError(validationFeatures160[0], validationFeatures160[1], resultClosed160)\n",
    "errorTrained160 = meanSquareError(trainFeatures160[0], trainFeatures160[1], resultClosed160)\n",
    "# print('result of closed form: \\n', resultClosed160)\n",
    "print('error of closed form: \\n', errorClosed160)\n",
    "print('error of closed trained: \\n', errorTrained160)\n",
    "len(trainFeatures160[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conclusion: 60 features are better than 160, validation error larger than closed form in 160: potentially overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra experiment: count word frequencies and eliminate stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tFreqS = countAllFrequencyStopWord(train)\n",
    "trainFeatures60S = parseFeatures(data[:10000], lambda u, v: wordCountWithStopwords(u, v, tFreqS), 60, ['children', 'controv', 'isRoot', 'text'])\n",
    "trainFeatures160S = parseFeatures(data[:10000], lambda u, v: wordCountWithStopwords(u, v, tFreqS), 160, ['children', 'controv', 'isRoot', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validationFeatures60S = parseFeatures(validation, lambda u, v: wordCountWithStopwords(u, v, tFreqS), 60, ['children', 'controv', 'isRoot', 'text'])\n",
    "validationFeatures160S = parseFeatures(validation, lambda u, v: wordCountWithStopwords(u, v, tFreqS), 160, ['children', 'controv', 'isRoot', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultClosed60S = closedFormLinearRegression(trainFeatures60S[0], trainFeatures60S[1])\n",
    "errorClosed60S = meanSquareError(validationFeatures60S[0], validationFeatures60S[1], resultClosed60S)\n",
    "errorTrained60S = meanSquareError(trainFeatures60S[0], trainFeatures60S[1], resultClosed60S)\n",
    "# print('result of closed form: \\n', resultClosed60S)\n",
    "print('error of closed form: \\n', errorClosed60S)\n",
    "print('error of closed trained: \\n', errorTrained60S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultClosed160S = closedFormLinearRegression(trainFeatures160S[0], trainFeatures160S[1])\n",
    "errorClosed160S = meanSquareError(validationFeatures160S[0], validationFeatures160S[1], resultClosed160S)\n",
    "errorTrained160S = meanSquareError(trainFeatures160S[0], trainFeatures160S[1], resultClosed160S)\n",
    "# print('result of closed form: \\n', resultClosed160S)\n",
    "print('error of closed form: \\n', errorClosed160S)\n",
    "print('error of closed trained: \\n', errorTrained160S)\n",
    "len(trainFeatures160[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conclusion: eliminating stopwords does not gives better result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with new features: fraction of noun and existence of URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word pos count, remove punc and stopwords to imporve model\n",
    "# @Param: singleText: text to process\n",
    "# @Return: vector of [verbcount, nouncount, adjcount]\n",
    "def wordPOSCountWithStopwords(singleText): \n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    withoutPunc = tokenizer.tokenize(singleText)\n",
    "    setOfStop = set(stopwords.words())\n",
    "    tagged = nltk.pos_tag([s.lower() for s in withoutPunc if s.lower() not in setOfStop])\n",
    "    \n",
    "    verbTotal, nounTotal, adjTotal = 0, 0, 0\n",
    "    counts = Counter(tag for wordType, tag in tagged)\n",
    "    totalCount = len(singleText.split())\n",
    "    \n",
    "    for key, value in counts.items(): \n",
    "        if 'NN' in key: \n",
    "            nounTotal = nounTotal + 1\n",
    "        elif 'VB' in key: \n",
    "            verbTotal = verbTotal + 1\n",
    "        elif 'JJ' in key: \n",
    "            adjTotal = adjTotal + 1\n",
    "            \n",
    "    if totalCount > 0: \n",
    "        return [float(verbTotal) / totalCount, float(nounTotal) / totalCount, float(adjTotal) / totalCount]\n",
    "    else: \n",
    "        return [0.0, 0.0, 0.0]\n",
    "# Test: \n",
    "# print(wordPOSCountWithStopwords(data[3]['text']))\n",
    "# print(data[3]['text'])\n",
    "\n",
    "\n",
    "def hasURL(text): \n",
    "    if re.match(r\"(http://[^ ]+)\", text) != None: \n",
    "        return 1.0\n",
    "    else: \n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With fraction of noun in whole words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainMyFeatures60 = parseFeatures(train, lambda u, v: wordCountNaive(u, v, tFreq), 60, ['children', 'controv', 'isRoot', 'noun'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vFreq = countAllFrequencyNaive(validation)\n",
    "validationMyFeatures60 = parseFeatures(validation, lambda u, v: wordCountNaive(u, v, tFreq), 60, ['children', 'controv', 'isRoot', 'noun'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultMyClosed = closedFormLinearRegression(trainMyFeatures60[0], trainMyFeatures60[1])\n",
    "errorMyClosed = meanSquareError(validationMyFeatures60[0], validationMyFeatures60[1], resultMyClosed)\n",
    "errorMyTrained = meanSquareError(trainMyFeatures60[0], trainMyFeatures60[1], resultMyClosed)\n",
    "# print('result of closed form: \\n', resultMyClosed)\n",
    "print('error of closed form: \\n', errorMyClosed)\n",
    "print('error of closed trained: \\n', errorMyTrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainMyFeaturesURL60 = parseFeatures(train, lambda u, v: wordCountNaive(u, v, tFreq), 60, ['children', 'controv', 'isRoot', 'url'])\n",
    "validationMyFeaturesURL60 = parseFeatures(validation, lambda u, v: wordCountNaive(u, v, tFreq), 60, ['children', 'controv', 'isRoot', 'url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultMyClosedURL = closedFormLinearRegression(trainMyFeaturesURL60[0], trainMyFeaturesURL60[1])\n",
    "errorMyClosedURL = meanSquareError(validationMyFeaturesURL60[0], validationMyFeaturesURL60[1], resultMyClosedURL)\n",
    "errorMyTrainedURL = meanSquareError(trainMyFeaturesURL60[0], trainMyFeaturesURL60[1], resultMyClosedURL)\n",
    "# print('result of closed form: \\n', resultMyClosedURL)\n",
    "print('error of closed form: \\n', errorMyClosedURL)\n",
    "print('error of closed trained: \\n', errorMyTrainedURL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainMyFeaturesALL60 = parseFeatures(train, lambda u, v: wordCountNaive(u, v, tFreq), 60, ['children', 'controv', 'isRoot', 'noun', 'url'])\n",
    "validationMyFeaturesALL60 = parseFeatures(validation, lambda u, v: wordCountNaive(u, v, tFreq), 60, ['children', 'controv', 'isRoot', 'noun', 'url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultMyClosedALL = closedFormLinearRegression(trainMyFeaturesALL60[0], trainMyFeaturesALL60[1])\n",
    "errorMyClosedALL = meanSquareError(validationMyFeaturesALL60[0], validationMyFeaturesALL60[1], resultMyClosedALL)\n",
    "errorMyTrainedALL = meanSquareError(trainMyFeaturesALL60[0], trainMyFeaturesALL60[1], resultMyClosedALL)\n",
    "# print('result of closed form: \\n', resultMyClosedALL)\n",
    "print('error of closed form: \\n', errorMyClosedALL)\n",
    "print('error of closed trained: \\n', errorMyTrainedALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conclusion: adding extra features does not help with accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x4(noun) and x5(url) has no significant coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test of coefficient (Hastie. et, al)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "sm.OLS(trainMyFeaturesALL60[1], trainMyFeaturesALL60[0]).fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Improved!) Interaction Effect among basic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFeaturesI = parseTransformedFeatures(train, lambda u, v: wordCountNaive(u, v, tFreq), 0, ['children', 'controv', 'isRoot', 'child and controv', 'child and isRoot', 'all interacted', 'controv and isRoot'])\n",
    "validationFeaturesI = parseTransformedFeatures(validation, lambda u, v: wordCountNaive(u, v, tFreq), 0, ['children', 'controv', 'isRoot', 'child and controv', 'child and isRoot', 'all interacted', 'controv and isRoot']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultClosedI = closedFormLinearRegression(trainFeaturesI[0], trainFeaturesI[1])\n",
    "errorClosedI = meanSquareError(validationFeaturesI[0], validationFeaturesI[1], resultClosedI)\n",
    "errorTrainedI = meanSquareError(trainFeaturesI[0], trainFeaturesI[1], resultClosedI)\n",
    "print('result of closed form: \\n', resultClosedI)\n",
    "print('error of closed form: \\n', errorClosedI)\n",
    "print('error of closed trained: \\n', errorTrainedI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: use of g(x) = 1-exp(-0.04x) to transform children variable, and interact the transformed children with isRoot decreases loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.OLS(trainFeaturesI[1], trainFeaturesI[0]).fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conclusion: interaction terms are not significant (all p-values are not smaller than 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra experiment: Forward Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "trainMyFeatures60 = parseTransformedFeatures(train, lambda u, v: wordCountNaive(u, v, tFreq), 160, ['children', 'controv', 'isRoot', 'child and controv', 'text']) # , 'child and controv', 'text'\n",
    "validationMyFeatures60 = parseTransformedFeatures(validation, lambda u, v: wordCountNaive(u, v, tFreq), 160, ['children', 'controv', 'child and controv', 'isRoot', 'text'])\n",
    "testMyFeatures60 = parseTransformedFeatures(test, lambda u, v: wordCountNaive(u, v, tFreq), 160, ['children', 'controv', 'isRoot', 'child and controv', 'text'])\n",
    "# , 'noun', 'url'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(numpy.transpose(trainMyFeatures60[0])[6], numpy.transpose(trainMyFeatures60[1]), alpha = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf = pandas.DataFrame(trainMyFeatures60[0])\n",
    "validationdf = pandas.DataFrame(validationMyFeatures60[0])\n",
    "traindf.shape, validationdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fullPredictors = set([i for i in range(0, 165)])\n",
    "levelBest = [[sys.float_info.max, set()]]\n",
    "c = 0\n",
    "for k in range(1, len(fullPredictors)): \n",
    "    \n",
    "    prevBest = min([l for l in levelBest if len(l[1]) == k - 1], key = lambda x: x[0])[1]\n",
    "    currentPredictors = fullPredictors - prevBest\n",
    "    mseVal = []\n",
    "    \n",
    "    for predictor in currentPredictors: \n",
    "        selected = prevBest | set([predictor])\n",
    "        \n",
    "        trainX = traindf[list(selected)].values\n",
    "        validX = validationdf[list(selected)].values\n",
    "        trainY = trainMyFeatures60[1]\n",
    "        validY = validationMyFeatures60[1]\n",
    "        \n",
    "        try: \n",
    "            resultW = closedFormLinearRegression(trainX, trainY)\n",
    "        except: \n",
    "            continue\n",
    "        mseVal.append([meanSquareError(validX, validY, resultW), selected])\n",
    "    \n",
    "    currentBest = min(mseVal, key = lambda x: x[0])\n",
    "    \n",
    "    if len(mseVal) > 0: \n",
    "        levelBest.append(currentBest)\n",
    "        print(currentBest)\n",
    "    \n",
    "    # Process Bar\n",
    "    sys.stdout.write('\\r')\n",
    "    count = int((float(c) / float(len(fullPredictors))) * 100)\n",
    "    sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(count / 5), count))\n",
    "    sleep(0.001)\n",
    "    sys.stdout.flush()\n",
    "    c = c + 1\n",
    "    \n",
    "\n",
    "trainX = traindf[list(fullPredictors)].values\n",
    "validX = validationdf[list(fullPredictors)].values\n",
    "trainY = trainMyFeatures60[1]\n",
    "validY = validationMyFeatures60[1]\n",
    "ok = True\n",
    "try: \n",
    "    resultW = closedFormLinearRegression(trainX, trainY)\n",
    "except: \n",
    "    ok = False\n",
    "if ok: \n",
    "    mseVal.append([meanSquareError(validX, validY, resultW), selected])\n",
    "bestOfAll = sorted(levelBest, key = lambda v: v[0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('best set of predictors with interaction term: \\n', bestOfAll[0])\n",
    "print('error: \\n', bestOfAll[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultClosedFinal = closedFormLinearRegression(numpy.array(trainMyFeatures60[0])[:,list(bestOfAll[1])], trainMyFeatures60[1])\n",
    "meanSquareError(numpy.array(testMyFeatures60[0])[:,list(bestOfAll[1])], testMyFeatures60[1], resultClosedFinal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: overfits validation set (meta-overfitting)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
