{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import re\n",
    "import itertools\n",
    "from time import sleep\n",
    "import sys\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter\n",
    "with open(\"proj1_data.json\") as fp:\n",
    "    data = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word count, remove punc but not stopwords\n",
    "# @Param: singleText: text to process, numberOfFeatures: 0, 60, 160?\n",
    "# @Return: vector of count (x in description)\n",
    "def wordCountWithoutStopwords(singleText, numberOfFeatures): \n",
    "\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    withoutPunc = tokenizer.tokenize(singleText)\n",
    "    countDict = Counter(s.lower() for s in withoutPunc)\n",
    "        \n",
    "    i = 0\n",
    "    returnVector = [0.0 for i in range(0, numberOfFeatures)]\n",
    "    for key, value in countDict.most_common(numberOfFeatures): \n",
    "        if i == numberOfFeatures: \n",
    "            break\n",
    "        returnVector[i] = float(value)\n",
    "        i = i + 1\n",
    "    \n",
    "    return returnVector\n",
    "\n",
    "# Test: \n",
    "# print(wordCountWithoutStopwords([data[8]['text']], 160))\n",
    "# print(data[8]['text'])\n",
    "\n",
    "\n",
    "\n",
    "# word count, remove punc and stopwords to imporve model\n",
    "# @Param: singleText: text to process, numberOfFeatures: 0, 60, 160?\n",
    "# @Return: vector of count (x in description)\n",
    "def wordCountWithStopwords(singleText, numberOfFeatures): \n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    withoutPunc = tokenizer.tokenize(singleText)\n",
    "    countDict = Counter(s.lower() for s in withoutPunc if s.lower() not in stopwords.words())\n",
    "    \n",
    "    i = 0\n",
    "    returnVector = [0.0 for i in range(0, numberOfFeatures)]\n",
    "    for key, value in countDict.most_common(numberOfFeatures): \n",
    "        if i == numberOfFeatures: \n",
    "            break\n",
    "        returnVector[i] = float(value)\n",
    "        i = i + 1\n",
    "    \n",
    "    return returnVector\n",
    "\n",
    "# Test: \n",
    "# print(wordCountWithStopwords([data[8]['text']], 160))\n",
    "# print(data[8]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word pos count, remove punc but not stopwords\n",
    "# @Param: singleText: text to process\n",
    "# @Return: @Return: vector of [verbcount, nouncount, adjcount]\n",
    "def wordPOSCountWithoutStopwords(singleText): \n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    withoutPunc = tokenizer.tokenize(singleText)\n",
    "    tagged = nltk.pos_tag([s.lower() for s in withoutPunc])\n",
    "    \n",
    "    verbTotal, nounTotal, adjTotal = 0, 0, 0\n",
    "    counts = Counter(tag for wordType, tag in tagged)\n",
    "    totalCount = sum(counts.values())\n",
    "    \n",
    "    for key, value in counts.items(): \n",
    "        if 'NN' in key: \n",
    "            nounTotal = nounTotal + 1\n",
    "        elif 'VB' in key: \n",
    "            verbTotal = verbTotal + 1\n",
    "        elif 'JJ' in key: \n",
    "            adjTotal = adjTotal + 1\n",
    "            \n",
    "    if totalCount > 0: \n",
    "        return [float(verbTotal) / totalCount, float(nounTotal) / totalCount, float(adjTotal) / totalCount]\n",
    "    else: \n",
    "        return [0.0, 0.0, 0.0]\n",
    "# Test: \n",
    "# print(wordPOSCountWithoutStopwords(data[3]['text']))\n",
    "# print(data[3]['text'])\n",
    "\n",
    "\n",
    "\n",
    "# word pos count, remove punc and stopwords to imporve model\n",
    "# @Param: singleText: text to process\n",
    "# @Return: vector of [verbcount, nouncount, adjcount]\n",
    "def wordPOSCountWithStopwords(singleText): \n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    withoutPunc = tokenizer.tokenize(singleText)\n",
    "    tagged = nltk.pos_tag([s.lower() for s in withoutPunc if s.lower() not in stopwords.words()])\n",
    "    \n",
    "    verbTotal, nounTotal, adjTotal = 0, 0, 0\n",
    "    counts = Counter(tag for wordType, tag in tagged)\n",
    "    totalCount = sum(counts.values())\n",
    "    \n",
    "    for key, value in counts.items(): \n",
    "        if 'NN' in key: \n",
    "            nounTotal = nounTotal + 1\n",
    "        elif 'VB' in key: \n",
    "            verbTotal = verbTotal + 1\n",
    "        elif 'JJ' in key: \n",
    "            adjTotal = adjTotal + 1\n",
    "            \n",
    "    if totalCount > 0: \n",
    "        return [float(verbTotal) / totalCount, float(nounTotal) / totalCount, float(adjTotal) / totalCount]\n",
    "    else: \n",
    "        return [0.0, 0.0, 0.0]\n",
    "# Test: \n",
    "# print(wordPOSCountWithStopwords(data[3]['text']))\n",
    "# print(data[3]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasURL(text): \n",
    "    if re.match(r\"(http://[^ ]+)\", text) != None: \n",
    "        return 1.0\n",
    "    else: \n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pythonListTranspose(xl): \n",
    "    return list(map(list, itertools.zip_longest(*xl)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================  ] 90%\n",
      "[[0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.25, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.16666666666666666, 0.0, 0.16666666666666666, 0.0, 0.0], [0.0, 0.0, 0.0, 4.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 1.0, 0.14705882352941177, 0.058823529411764705, 0.029411764705882353, 0.0, 0.0], [0.0, 0.0, 0.0, 10.0, 6.0, 5.0, 5.0, 4.0, 4.0, 4.0, 3.0, 3.0, 3.0, 0.11363636363636363, 0.045454545454545456, 0.022727272727272728, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2222222222222222, 0.1111111111111111, 0.0, 0.0], [1.0, 1.0, 0.0, 3.0, 2.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09090909090909091, 0.18181818181818182, 0.09090909090909091, 0.0, 0.0], [1.0, 0.0, 0.0, 4.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.13333333333333333, 0.06666666666666667, 0.06666666666666667, 0.0, 0.0]]\n"
     ]
    }
   ],
   "source": [
    "def parseFeatures(dataVector, wordCountFunction, wordPOSFunction, numberOfTextFeatures, featureType): \n",
    "    y = []\n",
    "    childrenFeature = []\n",
    "    controversialityFeature = []\n",
    "    isRootFeature = []\n",
    "    processedTextFeature = []\n",
    "    verbFeature = []\n",
    "    nounFeature = []\n",
    "    adjFeature = []\n",
    "    urlFeature = []\n",
    "    crInteractionFeature = []\n",
    "    \n",
    "    c = 0\n",
    "    lenV = len(dataVector)\n",
    "    for dataPoint in dataVector: \n",
    "        y.append(float(dataPoint['popularity_score']))\n",
    "        numberOfChildren = float(dataPoint['children'])\n",
    "        childrenFeature.append(float(dataPoint['children']))\n",
    "        controversialityFeature.append(float(dataPoint['controversiality']))\n",
    "        if numberOfTextFeatures > 0: \n",
    "            processedTextFeature.append(wordCountFunction(dataPoint['text'], numberOfTextFeatures))\n",
    "        wordAnalysis = wordPOSFunction(dataPoint['text'])\n",
    "        verbFeature.append(wordAnalysis[0])\n",
    "        nounFeature.append(wordAnalysis[1])\n",
    "        adjFeature.append(wordAnalysis[2])\n",
    "        isRootVar = -1.0\n",
    "        if data[0]['is_root']: \n",
    "            isRootVar = 1.0\n",
    "            isRootFeature.append(1.0)\n",
    "        else: \n",
    "            isRootVar = 0.0\n",
    "            isRootFeature.append(0.0)\n",
    "        urlFeature.append(hasURL(dataPoint['text']))\n",
    "        crInteractionFeature.append(isRootVar * numberOfChildren)\n",
    "        \n",
    "        sys.stdout.write('\\r')\n",
    "        count = int((float(c) / float(lenV)) * 100)\n",
    "        sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(count / 5), count))\n",
    "        sys.stdout.flush()\n",
    "        c = c + 1\n",
    "    \n",
    "    mergedBasicFeatures = [childrenFeature, controversialityFeature, isRootFeature]\n",
    "    if numberOfTextFeatures > 0: \n",
    "        mergedTextFeatures = mergedBasicFeatures + list(map(list, zip(*(processedTextFeature))))\n",
    "    else: \n",
    "        mergedTextFeatures = mergedBasicFeatures\n",
    "    mergedExtraFeatures = mergedTextFeatures + [verbFeature, nounFeature, adjFeature, urlFeature, crInteractionFeature]\n",
    "    mergedNoTextExtraFeatures = mergedBasicFeatures + [verbFeature, nounFeature, adjFeature, urlFeature, crInteractionFeature]\n",
    "    \n",
    "    if featureType == 'basic': \n",
    "        return mergedBasicFeatures, y\n",
    "    elif featureType == 'text': \n",
    "        return mergedTextFeatures, y\n",
    "    elif featureType == 'extra':\n",
    "        return mergedExtraFeatures, y\n",
    "    elif featureType == 'extraOnly': \n",
    "        return mergedNoTextExtraFeatures, y\n",
    "    else: \n",
    "        return mergedBasicFeatures, y\n",
    "\n",
    "crazyFeatures = parseFeatures(data[:10], wordCountWithoutStopwords, wordPOSCountWithStopwords, 10, 'extra')\n",
    "crazyFeaturesX = pythonListTranspose(crazyFeatures[0])\n",
    "crazyFeaturesY = crazyFeatures[1]\n",
    "print()\n",
    "print(crazyFeaturesX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'ITS RAINING SIDEWAYS',\n",
       " 'is_root': False,\n",
       " 'controversiality': 0,\n",
       " 'children': 0,\n",
       " 'popularity_score': 1.254698160267241}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
